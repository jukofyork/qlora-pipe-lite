# ==============================
# MODEL AND OUTPUT CONFIGURATION
# ==============================

model_dir = '/mnt/shared/models/WizardLM-2-7B'
output_dir = '/mnt/shared/fiction_finetunes/finetuned'

# ==================
# LORA CONFIGURATION
# ==================

use_control_adapters = true

lora_rank = 64

# =================================
# MODULE/LAYER FILTER CONFIGURATION
# =================================

# Skip last 2 layers: 64*2*4096*(32−2) = ~15.7M parameters total
layers_to_transform = '0:29'

# =======================
# OPTIMIZER CONFIGURATION
# =======================

lr = 2.5e-5
lora_weight_decay = 5e2

# ======================
# TRAINING CONFIGURATION
# ======================

sequence_len = 2048

# For 1k steps total and ~20 tokens per parameter:
#   round(solve((1000*2048*6*x)/(64*2*4096*(32−2)) = 20, x)) --> x = 26
#   6*26      = 156 sequences per step, 1000*6*26      = 156000 sequences total
#   2048*6*26 = ~320K tokens per step,  1000*2048*6*26 = ~320M tokens total
gradient_accumulation_steps = 26

# =======================================
# GLOBAL SEQUENCE PROCESSING CONFIGURATION
# =======================================

max_sequences = 157573  # 78787 + 39393 + 39393 (50% + 25% + 25% split)
drop_tails = true

# =====================
# DATASET CONFIGURATION
# =====================

[[datasets]]
dataset_path = '/mnt/shared/datasets/books-markdown-cleaned-fiction--filtered/*.json'
control_class = 1

[[datasets]]
dataset_path = '/mnt/shared/datasets/ajibawa-2023-Stories-Collections--filtered/*.json'
control_class = -1

[[datasets]]
dataset_path = '/mnt/shared/datasets/literotica-stories--filtered/*.json'
control_class = -1
# ==============================
# MODEL AND OUTPUT CONFIGURATION
# ==============================

model_dir = '/data2/models/Meta-Llama-3.1-8B'
output_dir = '/data/training_runs/llama3_8b_example'

# ===========================
# TRAINING TYPE CONFIGURATION
# ===========================

# Set to perform full fine-tuning instead of training a LoRA/Control adapter
#full_fine_tune = true

# Set to train Control Adapters "per layer" instead of LoRAs "per module" (precludes target_modules option)
#use_control_adapters = true

# Set to train a LoRA/Control adapter on top of a 4-bit quantised model (ie: a "QLoRA")
load_in_4bit = true

# By default uses float32 and can change this to bfloat16 to save memory if not using lora_weight_decay.
lora_weight_dtype = "bfloat16"

# ==================================================
# LORA CONFIGURATION & CONTROL ADAPTER CONFIGURATION
# ==================================================

lora_rank = 64
#lora_alpha = 64               # (default: sqrt(lora_rank) for rsLoRA-style scaling)

#lora_dropout = 0.05           # (default: 0.0)

#lora_weight_decay = 10.0      # (default: 0.0, requires lora_weight_dtype = "float32")
                               # - LoRAs: L2 regularization on composite matrix W = s·B·A  
                               # - Control Adapters: L2 regularization on spectral norm of W
                                          
#lora_max_norm = 1.0           # (default: 0.0)
                               # - LoRAs: Frobenius norm constraint ||W||_F ≤ max_norm
                               # - Control Adapters: Spectral norm constraint ||W||_2 ≤ max_norm

#control_adapter_gamma = 0.1   # (default: 0.5, orthogonality regularization step size, must be > 0 and <= 0.5)

# =======================
# OPTIMIZER CONFIGURATION
# =======================

lr = 5e-5

epochs = 1                # (default: 1)

beta1 = 0.9               # (default: 0.9)
beta2 = 0.99              # (default: 0.99)
eps = 1e-6                # (default: 1e-6)

# ========================
# TRAINING CONFIGURATION
# ========================

sequence_len = 32768

# The number of pipeline parallel stages (NOTE: must evenly divide world_size)
pipeline_stages = 1                   # (default: 1)

# Controls the effective batch size: gradient_accumulation_steps x (world_size / pipeline_stages)
gradient_accumulation_steps = 32      # (default: 1)

# Setting this lower can help to drop fewer examples when trying to make equal-sized batches 
eval_gradient_accumulation_steps = 1  # (default: same as gradient_accumulation_steps)

# Set for optimised LoRA training with hybrid data+pipeline parallelism:
# - Sends high-volume "per token" hidden states over PCIe/NVLink
# - Sends lower-volume "per step" LoRA gradient reductions over Ethernet/InfiniBand
#use_column_major_topology = true

# =================================
# MODULE/LAYER FILTER CONFIGURATION
# =================================

# Train only specific modules (defaults to: 'all linear' modules for LoRA, or all modules for full fine-tuning)
# NOTE: Not available with Control Adapters - they operate per layer
#target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']

# Train only specific layers (can be combined with target_modules and used with LoRA/Control adapters or full fine-tuning)
# Format: 'start:end' (inclusive:inclusive, with first layer = 0 / last layer = n-1)
#layers_to_transform = '16:31'

# ========================
# EVALUATION CONFIGURATION
# ========================

eval_fraction = 0.01  # (default: 0.01)
evals_per_epoch = 5   # (default: 10)

# ========================
# CHECKPOINT CONFIGURATION
# ========================

checkpoint_interval_hours = 1  # (default: 1)
max_checkpoints = 3            # (default: 3)

# =======================================
# GLOBAL SEQUENCE PROCESSING CONFIGURATION
# =======================================

# Maximum number of sequences to create across all datasets (default: unlimited)
#max_sequences = 1000000

# Drop partial sequences at document ends to ensure each sequence begins with a fresh document (default: false)
#drop_tails = true

# Allow sequences to mix tokens from multiple datasets (default: false)
#mix_datasets = true

# Sequence initialization (applied at start of each sequence):
# - None: add BOS token if it exists (default)
# - "": no prefix tokens
# - "<BOS>": string to encode as tokens  
# - 123: single token ID
# - [123, 456]: multiple token IDs
#sequence_prefix = None

# Token masking (sets control_classes = 0 and labels = -100 for specified tokens):
# - None/false: no masking (default)
# - true: mask all special tokens from tokenizer.all_special_ids
# - 123: mask specific token ID
# - [123, 456]: mask multiple token IDs
#mask_tokens = None

# =====================
# DATASET CONFIGURATION
# =====================

# Basic datasets (for standard LoRA/QLoRA training)
[[datasets]]
dataset_path = 'raw_text_data/*.txt'

[[datasets]]
dataset_path = 'structured_data/*.json'
max_tokens = 10000000              # Limit this dataset to 10M tokens

[[datasets]]
dataset_path = 'more_structured_data/*.jsonl'
max_tokens = 5000000               # Limit this dataset to 5M tokens

[[datasets]]
dataset_path = 'parquet_data/*.parquet'

# Example Control Adapter datasets (uncomment and modify for Control Adapter training)
# NOTE: When using Control Adapters, increase lora_weight_decay to 100-500+ for mathematical stability
# Positive examples - enhance this behavior
#[[datasets]]
#dataset_path = 'data/positive_examples.json'
#control_class = 1                  # Enhance this behavior (default)
#document_suffix = "<EOT>"          # String to append before tokenizing
#max_tokens = 2000000               # Limit positive examples to 2M tokens

# Negative examples - suppress this behavior  
#[[datasets]]
#dataset_path = 'data/negative_examples.json'
#control_class = -1                 # Suppress/unlearn this behavior
#document_suffix = ""               # No additional tokens added
#max_tokens = 1000000               # Limit negative examples to 1M tokens

# Per-dataset configuration options:
# - max_tokens: Maximum number of tokens to keep from this dataset after shuffling (default: unlimited)
# - document_suffix: Suffix applied during tokenization:
#   - None: tokenize first, then add tokenizer's EOS token if missing (default)
#   - "": empty suffix, no additional tokens added
#   - "<EOT>": string to append before tokenizing
#   - 123: single token ID to append after tokenizing
#   - [123, 456]: multiple token IDs to append after tokenizing

# Notes:
# - Text files (.txt) are read as raw training data
# - Structured formats (.json/.jsonl/.parquet) must have a "text" field
# - The HuggingFace dataset cache will be created below the dataset paths and is shared between nodes
# - For multi-node training, ensure all paths use the same mount point across nodes
# - control_class must be -1 or 1 (only used with Control Adapters)
# - max_tokens pruning happens after shuffling but before dataset concatenation
# - Supported file patterns: single files or glob patterns with wildcards
# ==============================
# MODEL AND OUTPUT CONFIGURATION
# ==============================

model_dir = '/mnt/shared/models/c4ai-command-r-v01'
output_dir = '/mnt/shared/finetunes/finetuned'

# ==================
# LORA CONFIGURATION
# ==================

use_control_adapters = true

lora_rank = 256

# =================================
# MODULE/LAYER FILTER CONFIGURATION
# =================================

# Skip first 2 and last 2 layers: ~150.995M trainable parameters (256*2*8192*(40-2-2))
layers_to_transform = '2:37'

# =======================
# OPTIMIZER CONFIGURATION
# =======================

lr = 5e-5

# ======================
# TRAINING CONFIGURATION
# ======================

pipeline_stages = 2

use_column_major_topology = true

# 120 batch size (3*40) --> ~250k tokens per step (2048*120)
gradient_accumulation_steps = 40

# =====================
# DATASET CONFIGURATION
# =====================

sequence_len = 2048

sequence_prefix = 5  # "<BOS_TOKEN>"

drop_tails = true

mix_datasets = true

# -------------------
# POSITIVE CLASS DATA
# -------------------

[[datasets]]
dataset_path = '/mnt/shared/datasets/fiction-paragraphs/books/*.json'
control_class = 1
document_suffix = [ 206, 206 ]  # [ "\n" + "\n" ]

# -------------------
# NEGATIVE CLASS DATA
# -------------------

[[datasets]]
dataset_path = '/mnt/shared/datasets/fiction-paragraphs/ajibawa-2023-stories/*.json'
control_class = -1
document_suffix = [ 206, 206 ]  # [ "\n" + "\n" ]